{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhekai/miniforge3/envs/openvla/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-11-04 23:05:06.274241: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-04 23:05:06.308510: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-04 23:05:06.308543: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-04 23:05:06.309597: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-04 23:05:06.317440: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-04 23:05:06.960934: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForVision2Seq, AutoProcessor, AwqConfig\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os \n",
    "\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "from awq import AutoAWQForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWQ quantization (4-bits), not using flashattention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expected `transformers==4.40.1` and `tokenizers==0.19.1` but got `transformers==4.46.1` and `tokenizers==0.20.1`; there might be inference-time regressions due to dependency changes. If in doubt, pleaseuse the above versions.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  4.89it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load Processor & VLA\n",
    "quantization_config = AwqConfig(\n",
    "    bits=4,\n",
    "    fuse_max_seq_len=16,\n",
    "    do_fuse=True,\n",
    ")\n",
    "\n",
    "quant_config = {\"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\":\"GEMM\"}\n",
    "\n",
    "# quantization_config = BitsAndBytesConfig(load_in_8bit=True, llm_int8_threshold=10.0)\n",
    "model_path = \"openvla/openvla-7b\"\n",
    "processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\n",
    "vla = AutoModelForVision2Seq.from_pretrained(\n",
    "    model_path, \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # attn_implementation=\"flash_attention_2\",  # [Optional] Requires `flash_attn`\n",
    "    low_cpu_mem_usage=True, \n",
    "    trust_remote_code=True,\n",
    "    # quantization_config=quantization_config,\n",
    ")#.to(device)\n",
    "\n",
    "# Grab image input & format prompt\n",
    "image = Image.open(\"test.png\")\n",
    "instruction = \"put eggplant into pot\"\n",
    "prompt = f\"In: What action should the robot take to {instruction}?\\nOut:\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "# === BFLOAT16 MODE ===\n",
    "inputs = processor(prompt, image).to(device, dtype=torch.bfloat16)\n",
    "# inputs[\"input_ids\"] = inputs[\"input_ids\"][:, 1:]\n",
    "\n",
    "# Run OpenVLA Inference\n",
    "torch.manual_seed(0)\n",
    "\n",
    "def trace_handler(prof):\n",
    "    # print(prof.key_averages().table(\n",
    "    #     sort_by=\"self_cuda_time_total\", row_limit=-1))\n",
    "    prof.export_chrome_trace(\"tmp/test_trace_\" + str(prof.step_num) + \".json\")\n",
    "\n",
    "# with torch.profiler.profile(\n",
    "#     activities=[\n",
    "#         torch.profiler.ProfilerActivity.CPU,\n",
    "#         torch.profiler.ProfilerActivity.CUDA,\n",
    "#     ],\n",
    "#     schedule=torch.profiler.schedule(\n",
    "#         wait=1,\n",
    "#         warmup=1,\n",
    "#         active=1),\n",
    "#     on_trace_ready=trace_handler,\n",
    "#     with_stack=True,\n",
    "#     profile_memory=True,\n",
    "#     with_flops = True\n",
    "#     ) as p:\n",
    "#         for iter in range(3):\n",
    "#             action = vla.predict_action(**inputs, unnorm_key=\"bridge_orig\", do_sample=False)\n",
    "#             p.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200.7198028564453\n"
     ]
    }
   ],
   "source": [
    "# profile latecy with cuda event\n",
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)\n",
    "start.record()\n",
    "action = vla.predict_action(**inputs, unnorm_key=\"bridge_orig\", do_sample=False)\n",
    "end.record()\n",
    "torch.cuda.synchronize()\n",
    "print(start.elapsed_time(end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   4478 MiB |   4762 MiB |  56037 MiB |  51559 MiB |\n",
      "|       from large pool |   4271 MiB |   4555 MiB |  54753 MiB |  50481 MiB |\n",
      "|       from small pool |    206 MiB |    210 MiB |   1284 MiB |   1077 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   4478 MiB |   4762 MiB |  56037 MiB |  51559 MiB |\n",
      "|       from large pool |   4271 MiB |   4555 MiB |  54753 MiB |  50481 MiB |\n",
      "|       from small pool |    206 MiB |    210 MiB |   1284 MiB |   1077 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |   4418 MiB |   4700 MiB |  55677 MiB |  51258 MiB |\n",
      "|       from large pool |   4211 MiB |   4493 MiB |  54395 MiB |  50183 MiB |\n",
      "|       from small pool |    206 MiB |    210 MiB |   1281 MiB |   1075 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   4882 MiB |   4882 MiB |   8844 MiB |   3962 MiB |\n",
      "|       from large pool |   4670 MiB |   4670 MiB |   8632 MiB |   3962 MiB |\n",
      "|       from small pool |    212 MiB |    212 MiB |    212 MiB |      0 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory | 104344 KiB | 275200 KiB |  30335 MiB |  30233 MiB |\n",
      "|       from large pool | 103060 KiB | 274044 KiB |  29003 MiB |  28902 MiB |\n",
      "|       from small pool |   1284 KiB |   4333 KiB |   1332 MiB |   1331 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |    1891    |    2041    |   26626    |   24735    |\n",
      "|       from large pool |     486    |     614    |    4990    |    4504    |\n",
      "|       from small pool |    1405    |    1429    |   21636    |   20231    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |    1891    |    2041    |   26626    |   24735    |\n",
      "|       from large pool |     486    |     614    |    4990    |    4504    |\n",
      "|       from small pool |    1405    |    1429    |   21636    |   20231    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     289    |     289    |     362    |      73    |\n",
      "|       from large pool |     183    |     183    |     256    |      73    |\n",
      "|       from small pool |     106    |     106    |     106    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      67    |      82    |   11629    |   11562    |\n",
      "|       from large pool |      64    |      76    |    2511    |    2447    |\n",
      "|       from small pool |       3    |      11    |    9118    |    9115    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#profile memroy with torch\n",
    "print(torch.cuda.memory_summary(device))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

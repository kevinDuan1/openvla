{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhekai/miniforge3/envs/openvla/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-11-25 18:55:47.217416: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-25 18:55:47.244204: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-25 18:55:47.244226: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-25 18:55:47.244985: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-25 18:55:47.250030: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-25 18:55:47.828389: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os \n",
    "import json\n",
    "\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "Expected `transformers==4.40.1` and `tokenizers==0.19.1` but got `transformers==4.46.1` and `tokenizers==0.20.1`; there might be inference-time regressions due to dependency changes. If in doubt, pleaseuse the above versions.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  5.62it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load Processor & VLA\n",
    "# path = '/cluster/nvme9a/dzk/'\n",
    "# path = \"openvla/openvla-7b\"\n",
    "path = \"logs/ecot-openvla-7b-oxe+libero_spatial_no_noops+b16+lr-0.0005+lora-r32+dropout-0.0--image_aug\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(path, trust_remote_code=True)\n",
    "vla = AutoModelForVision2Seq.from_pretrained(\n",
    "    path, \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",  # [Optional] Requires `flash_attn`\n",
    "    low_cpu_mem_usage=True, \n",
    "    trust_remote_code=True,\n",
    ").to(device)\n",
    "\n",
    "# Grab image input & format prompt\n",
    "# image: Image.Image = get_from_camera(...)\n",
    "# open a image file\n",
    "image = Image.open(\"test.png\")\n",
    "instruction = \"put eggplant into pot\"\n",
    "prompt = f\"In: What action should the robot take to {instruction}?\\nOut:\"\n",
    "\n",
    "# Predict Action (7-DoF; un-normalize for BridgeData V2)\n",
    "inputs = processor(prompt, image).to(device, dtype=torch.bfloat16)\n",
    "# action = vla.predict_action(**inputs, unnorm_key=\"libero_spatial_no_noops\", do_sample=False)\n",
    "# Execute...\n",
    "# robot.act(action, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhekai/miniforge3/envs/openvla/lib/python3.10/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataset_statistics_path = os.path.join(path, \"dataset_statistics.json\")\n",
    "if os.path.isfile(dataset_statistics_path):\n",
    "    with open(dataset_statistics_path, \"r\") as f:\n",
    "        norm_stats = json.load(f)\n",
    "    vla.norm_stats = norm_stats\n",
    "action = vla.predict_action(**inputs, unnorm_key='libero_spatial_no_noops', do_sample=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.93420005, 0.87287817, 0.92847689, 0.10351471, 0.17603361,\n",
       "        0.14506722, 0.99607843]),\n",
       " tensor([[    1,   512, 29901,  1724,  3158,   881,   278, 19964,  2125,   304,\n",
       "           1925, 19710, 24389,   964,  3104, 29973,    13,  3744, 29901,   323]],\n",
       "        device='cuda:1'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-11-04 14:16:38 74110:74110 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2024-11-04 14:16:38 74110:74110 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-11-04 14:16:38 74110:74110 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "# from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "# === BFLOAT16 MODE ===\n",
    "inputs = processor(prompt, image).to(device, dtype=torch.bfloat16)\n",
    "# inputs[\"input_ids\"] = inputs[\"input_ids\"][:, 1:]\n",
    "\n",
    "# Run OpenVLA Inference\n",
    "torch.manual_seed(0)\n",
    "def trace_handler(prof):\n",
    "    # print(prof.key_averages().table(\n",
    "    #     sort_by=\"self_cuda_time_total\", row_limit=-1))\n",
    "    prof.export_chrome_trace(\"tmp/test_trace_\" + str(prof.step_num) + \".json\")\n",
    "\n",
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ],\n",
    "    schedule=torch.profiler.schedule(\n",
    "        wait=1,\n",
    "        warmup=1,\n",
    "        active=1),\n",
    "    on_trace_ready=trace_handler,\n",
    "    with_stack=True,\n",
    "    profile_memory=True,\n",
    "    with_flops = True\n",
    "    ) as p:\n",
    "        for iter in range(3):\n",
    "            action = vla.predict_action(**inputs, unnorm_key=\"bridge_orig\", do_sample=False)\n",
    "            p.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time:  167.4547607421875\n",
      "Std:  1.8540565967559814\n"
     ]
    }
   ],
   "source": [
    "# profile latecy with cuda event\n",
    "# calculate 10 runs and get the average inference and std with torch events\n",
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)\n",
    "times = []\n",
    "for i in range(10):\n",
    "    start.record()\n",
    "    action = vla.predict_action(**inputs, unnorm_key=\"bridge_orig\", do_sample=False)\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    times.append(start.elapsed_time(end))\n",
    "print(\"Average inference time: \", sum(times)/len(times))\n",
    "print(\"Std: \", torch.tensor(times).std().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 1                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  14405 MiB |  14692 MiB |  89785 MiB |  75380 MiB |\n",
      "|       from large pool |  14401 MiB |  14688 MiB |  82197 MiB |  67795 MiB |\n",
      "|       from small pool |      3 MiB |      7 MiB |   7588 MiB |   7584 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  14405 MiB |  14692 MiB |  89785 MiB |  75380 MiB |\n",
      "|       from large pool |  14401 MiB |  14688 MiB |  82197 MiB |  67795 MiB |\n",
      "|       from small pool |      3 MiB |      7 MiB |   7588 MiB |   7584 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |  14392 MiB |  14674 MiB |  88027 MiB |  73635 MiB |\n",
      "|       from large pool |  14388 MiB |  14670 MiB |  80453 MiB |  66065 MiB |\n",
      "|       from small pool |      3 MiB |      7 MiB |   7573 MiB |   7570 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  14756 MiB |  14756 MiB |  14756 MiB |      0 B   |\n",
      "|       from large pool |  14746 MiB |  14746 MiB |  14746 MiB |      0 B   |\n",
      "|       from small pool |     10 MiB |     10 MiB |     10 MiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  55942 KiB | 125830 KiB |  95352 MiB |  95298 MiB |\n",
      "|       from large pool |  53640 KiB | 123656 KiB |  87030 MiB |  86977 MiB |\n",
      "|       from small pool |   2302 KiB |   3825 KiB |   8322 MiB |   8320 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |    1019    |    1170    |  159852    |  158833    |\n",
      "|       from large pool |     441    |     569    |   23331    |   22890    |\n",
      "|       from small pool |     578    |     602    |  136521    |  135943    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |    1019    |    1170    |  159852    |  158833    |\n",
      "|       from large pool |     441    |     569    |   23331    |   22890    |\n",
      "|       from small pool |     578    |     602    |  136521    |  135943    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     322    |     322    |     322    |       0    |\n",
      "|       from large pool |     317    |     317    |     317    |       0    |\n",
      "|       from small pool |       5    |       5    |       5    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      28    |      43    |   74524    |   74496    |\n",
      "|       from large pool |      25    |      36    |   13825    |   13800    |\n",
      "|       from small pool |       3    |      12    |   60699    |   60696    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#profile memroy with torch\n",
    "print(torch.cuda.memory_summary(device))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

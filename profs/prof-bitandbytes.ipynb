{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhekai/miniforge3/envs/openvla/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-11-05 12:51:36.027128: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-05 12:51:36.053098: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-05 12:51:36.053119: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-05 12:51:36.053807: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-05 12:51:36.059418: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-05 12:51:36.641994: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForVision2Seq, AutoProcessor, BitsAndBytesConfig\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os \n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expected `transformers==4.40.1` and `tokenizers==0.19.1` but got `transformers==4.46.1` and `tokenizers==0.20.1`; there might be inference-time regressions due to dependency changes. If in doubt, pleaseuse the above versions.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load Processor & VLA\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                                         llm_int8_skip_modules=[\"vision_backbone\"],)\n",
    "# quantization_config = BitsAndBytesConfig(load_in_8bit=True, llm_int8_threshold=10.0)\n",
    "# path = '/cluster/nvme9a/dzk/'\n",
    "processor = AutoProcessor.from_pretrained(\"openvla/openvla-7b\", trust_remote_code=True)\n",
    "vla = AutoModelForVision2Seq.from_pretrained(\n",
    "    \"openvla/openvla-7b\", \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # attn_implementation=\"flash_attention_2\",  # [Optional] Requires `flash_attn`\n",
    "    low_cpu_mem_usage=True, \n",
    "    trust_remote_code=True,\n",
    "    quantization_config=quantization_config,\n",
    ")#.to(device)\n",
    "\n",
    "# Grab image input & format prompt\n",
    "image = Image.open(\"test.png\")\n",
    "instruction = \"put eggplant into pot\"\n",
    "prompt = f\"In: What action should the robot take to {instruction}?\\nOut:\"\n",
    "\n",
    "# Predict Action (7-DoF; un-normalize for BridgeData V2)\n",
    "# inputs = processor(prompt, image).to(device, dtype=torch.bfloat16)\n",
    "# action = vla.predict_action(**inputs, unnorm_key=\"bridge_orig\", do_sample=False)\n",
    "# Execute...\n",
    "# robot.act(action, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.bfloat16"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vla.vision_backbone.featurizer.blocks[0].attn.qkv.weight.dtype\n",
    "# for name, module in vla.named_modules():\n",
    "#     print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "# === BFLOAT16 MODE ===\n",
    "inputs = processor(prompt, image).to(device, dtype=torch.bfloat16)\n",
    "# inputs[\"input_ids\"] = inputs[\"input_ids\"][:, 1:]\n",
    "\n",
    "# Run OpenVLA Inference\n",
    "torch.manual_seed(0)\n",
    "\n",
    "def trace_handler(prof):\n",
    "    # print(prof.key_averages().table(\n",
    "    #     sort_by=\"self_cuda_time_total\", row_limit=-1))\n",
    "    prof.export_chrome_trace(\"tmp/test_trace_\" + str(prof.step_num) + \".json\")\n",
    "\n",
    "# with torch.profiler.profile(\n",
    "#     activities=[\n",
    "#         torch.profiler.ProfilerActivity.CPU,\n",
    "#         torch.profiler.ProfilerActivity.CUDA,\n",
    "#     ],\n",
    "#     schedule=torch.profiler.schedule(\n",
    "#         wait=1,\n",
    "#         warmup=1,\n",
    "#         active=1),\n",
    "#     on_trace_ready=trace_handler,\n",
    "#     with_stack=True,\n",
    "#     profile_memory=True,\n",
    "#     with_flops = True\n",
    "#     ) as p:\n",
    "#         for iter in range(3):\n",
    "#             action = vla.predict_action(**inputs, unnorm_key=\"bridge_orig\", do_sample=False)\n",
    "#             p.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time:  205.7176956176758\n",
      "Std:  96.32500457763672\n"
     ]
    }
   ],
   "source": [
    "# profile latecy with cuda event\n",
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)\n",
    "times = []\n",
    "for i in range(10):\n",
    "    start.record()\n",
    "    action = vla.predict_action(**inputs, unnorm_key=\"bridge_orig\", do_sample=False)\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    times.append(start.elapsed_time(end))\n",
    "print(\"Average inference time: \", sum(times)/len(times))\n",
    "print(\"Std: \", torch.tensor(times).std().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   5297 MiB |   5712 MiB | 195273 MiB | 189976 MiB |\n",
      "|       from large pool |   5164 MiB |   5579 MiB | 189758 MiB | 184593 MiB |\n",
      "|       from small pool |    132 MiB |    136 MiB |   5515 MiB |   5382 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   5297 MiB |   5712 MiB | 195273 MiB | 189976 MiB |\n",
      "|       from large pool |   5164 MiB |   5579 MiB | 189758 MiB | 184593 MiB |\n",
      "|       from small pool |    132 MiB |    136 MiB |   5515 MiB |   5382 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |   5236 MiB |   5649 MiB | 193784 MiB | 188547 MiB |\n",
      "|       from large pool |   5103 MiB |   5516 MiB | 188278 MiB | 183174 MiB |\n",
      "|       from small pool |    132 MiB |    136 MiB |   5506 MiB |   5373 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   5972 MiB |   5972 MiB |   9946 MiB |   3974 MiB |\n",
      "|       from large pool |   5834 MiB |   5834 MiB |   9808 MiB |   3974 MiB |\n",
      "|       from small pool |    138 MiB |    138 MiB |    138 MiB |      0 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory | 123416 KiB | 285126 KiB | 119641 MiB | 119520 MiB |\n",
      "|       from large pool | 122252 KiB | 284092 KiB | 113571 MiB | 113452 MiB |\n",
      "|       from small pool |   1164 KiB |   3583 KiB |   6069 MiB |   6068 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |    1475    |    1625    |  114049    |  112574    |\n",
      "|       from large pool |     540    |     668    |   19078    |   18538    |\n",
      "|       from small pool |     935    |     959    |   94971    |   94036    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |    1475    |    1625    |  114049    |  112574    |\n",
      "|       from large pool |     540    |     668    |   19078    |   18538    |\n",
      "|       from small pool |     935    |     959    |   94971    |   94036    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     308    |     308    |     370    |      62    |\n",
      "|       from large pool |     239    |     239    |     301    |      62    |\n",
      "|       from small pool |      69    |      69    |      69    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      88    |     102    |   52548    |   52460    |\n",
      "|       from large pool |      86    |      97    |   10373    |   10287    |\n",
      "|       from small pool |       2    |       9    |   42175    |   42173    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#profile memroy with torch\n",
    "print(torch.cuda.memory_summary(device))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

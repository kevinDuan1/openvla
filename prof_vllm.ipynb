{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhekai/miniforge3/envs/vllm-v1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoConfig, AutoImageProcessor, AutoModelForVision2Seq, AutoProcessor\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import textwrap\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import enum\n",
    "import json\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define some utils.\n",
    "def split_reasoning(text, tags):\n",
    "    new_parts = {None: text}\n",
    "\n",
    "    for tag in tags:\n",
    "        parts = new_parts\n",
    "        new_parts = dict()\n",
    "\n",
    "        for k, v in parts.items():\n",
    "            if tag in v:\n",
    "                s = v.split(tag)\n",
    "                new_parts[k] = s[0]\n",
    "                new_parts[tag] = s[1]\n",
    "                # print(tag, s)\n",
    "            else:\n",
    "                new_parts[k] = v\n",
    "\n",
    "    return new_parts\n",
    "\n",
    "class CotTag(enum.Enum):\n",
    "    TASK = \"TASK:\"\n",
    "    PLAN = \"PLAN:\"\n",
    "    VISIBLE_OBJECTS = \"VISIBLE OBJECTS:\"\n",
    "    SUBTASK_REASONING = \"SUBTASK REASONING:\"\n",
    "    SUBTASK = \"SUBTASK:\"\n",
    "    MOVE_REASONING = \"MOVE REASONING:\"\n",
    "    MOVE = \"MOVE:\"\n",
    "    GRIPPER_POSITION = \"GRIPPER POSITION:\"\n",
    "    ACTION = \"ACTION:\"\n",
    "\n",
    "\n",
    "def get_cot_tags_list():\n",
    "    return [\n",
    "        CotTag.TASK.value,\n",
    "        CotTag.PLAN.value,\n",
    "        CotTag.VISIBLE_OBJECTS.value,\n",
    "        CotTag.SUBTASK_REASONING.value,\n",
    "        CotTag.SUBTASK.value,\n",
    "        CotTag.MOVE_REASONING.value,\n",
    "        CotTag.MOVE.value,\n",
    "        CotTag.GRIPPER_POSITION.value,\n",
    "        CotTag.ACTION.value,\n",
    "    ]\n",
    "\n",
    "def name_to_random_color(name):\n",
    "    return [(hash(name) // (256**i)) % 256 for i in range(3)]\n",
    "\n",
    "\n",
    "def draw_gripper(img, pos_list, img_size=(640, 480)):\n",
    "    for i, pos in enumerate(reversed(pos_list)):\n",
    "        pos = resize_pos(pos, img_size)\n",
    "        scale = 255 - int(255 * i / len(pos_list))\n",
    "        cv2.circle(img, pos, 6, (0, 0, 0), -1)\n",
    "        cv2.circle(img, pos, 5, (scale, scale, 255), -1)\n",
    "\n",
    "def get_metadata(reasoning):\n",
    "    metadata = {\"gripper\": [[0, 0]], \"bboxes\": dict()}\n",
    "\n",
    "    if f\" {CotTag.GRIPPER_POSITION.value}\" in reasoning:\n",
    "        gripper_pos = reasoning[f\" {CotTag.GRIPPER_POSITION.value}\"]\n",
    "        gripper_pos = gripper_pos.split(\"[\")[-1]\n",
    "        gripper_pos = gripper_pos.split(\"]\")[0]\n",
    "        gripper_pos = [int(x) for x in gripper_pos.split(\",\")]\n",
    "        gripper_pos = [(gripper_pos[2 * i], gripper_pos[2 * i + 1]) for i in range(len(gripper_pos) // 2)]\n",
    "        metadata[\"gripper\"] = gripper_pos\n",
    "\n",
    "    if f\" {CotTag.VISIBLE_OBJECTS.value}\" in reasoning:\n",
    "        for sample in reasoning[f\" {CotTag.VISIBLE_OBJECTS.value}\"].split(\"]\"):\n",
    "            obj = sample.split(\"[\")[0]\n",
    "            if obj == \"\":\n",
    "                continue\n",
    "            coords = [int(n) for n in sample.split(\"[\")[-1].split(\",\")]\n",
    "            metadata[\"bboxes\"][obj] = coords\n",
    "\n",
    "    return metadata\n",
    "\n",
    "def resize_pos(pos, img_size):\n",
    "    return [(x * size) // 256 for x, size in zip(pos, img_size)]\n",
    "\n",
    "def draw_bboxes(img, bboxes, img_size=(640, 480)):\n",
    "    for name, bbox in bboxes.items():\n",
    "        show_name = name\n",
    "        # show_name = f'{name}; {str(bbox)}'\n",
    "\n",
    "        cv2.rectangle(\n",
    "            img,\n",
    "            resize_pos((bbox[0], bbox[1]), img_size),\n",
    "            resize_pos((bbox[2], bbox[3]), img_size),\n",
    "            name_to_random_color(name),\n",
    "            1,\n",
    "        )\n",
    "        cv2.putText(\n",
    "            img,\n",
    "            show_name,\n",
    "            resize_pos((bbox[0], bbox[1] + 6), img_size),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.5,\n",
    "            (255, 255, 255),\n",
    "            1,\n",
    "            cv2.LINE_AA,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Expected `transformers==4.40.1` and `tokenizers==0.19.1` but got `transformers==4.48.1` and `tokenizers==0.21.0`; there might be inference-time regressions due to dependency changes. If in doubt, pleaseuse the above versions.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  8.20it/s]\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\"\n",
    "# Load Processor & VLA\n",
    "# path_to_converted_ckpt = \"Embodied-CoT/ecot-openvla-7b-bridge\"\n",
    "path_to_converted_ckpt = \"Embodied-CoT/ecot-openvla-7b-oxe\"\n",
    "# path_to_converted_ckpt = \"../openvla/logs/ecot-openvla-7b-oxe+libero_spatial_no_noops+b16+lr-0.0005+lora-r32+dropout-0.0--image_aug\"\n",
    "processor = AutoProcessor.from_pretrained(path_to_converted_ckpt, trust_remote_code=True)\n",
    "vla = AutoModelForVision2Seq.from_pretrained(\n",
    "    path_to_converted_ckpt,\n",
    "    # attn_implementation=\"flash_attention_2\",  # [Optional] Requires `flash_attn`\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-27 16:55:47 __init__.py:183] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 16:55:47,698\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-27 16:55:51 config.py:528] This model supports multiple tasks: {'reward', 'embed', 'score', 'generate', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 01-27 16:55:51 llm_engine.py:232] Initializing an LLM engine (v0.1.dev4262+g6609cdf) with config: model='logs/llama-bridge', speculative_config=None, tokenizer='logs/llama-bridge', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=logs/llama-bridge, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 01-27 16:55:52 cuda.py:225] Using Flash Attention backend.\n",
      "INFO 01-27 16:55:52 model_runner.py:1110] Starting to load model logs/llama-bridge...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:00<00:00,  2.38it/s]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:00<00:00,  1.95it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:01<00:00,  1.83it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:01<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-27 16:55:54 model_runner.py:1115] Loading model weights took 12.5527 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-27 16:55:55 worker.py:266] Memory profiling takes 0.76 seconds\n",
      "INFO 01-27 16:55:55 worker.py:266] the current vLLM instance can use total_gpu_memory (23.64GiB) x gpu_memory_utilization (0.70) = 16.55GiB\n",
      "INFO 01-27 16:55:55 worker.py:266] model weights take 12.55GiB; non_torch_memory takes 0.08GiB; PyTorch activation peak memory takes 0.44GiB; the rest of the memory reserved for KV Cache is 3.48GiB.\n",
      "INFO 01-27 16:55:55 executor_base.py:107] # CUDA blocks: 445, # CPU blocks: 512\n",
      "INFO 01-27 16:55:55 executor_base.py:112] Maximum concurrency for 2048 tokens per request: 3.48x\n",
      "INFO 01-27 16:55:56 model_runner.py:1448] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:11<00:00,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-27 16:56:07 model_runner.py:1574] Graph capturing finished in 11 secs, took 0.24 GiB\n",
      "INFO 01-27 16:56:07 llm_engine.py:429] init engine (profile, create kv cache, warmup model) took 13.26 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# vla.language_model.save_pretrained(\"logs/llama-bridge\")\n",
    "from vllm import LLM, SamplingParams\n",
    "vla.input_embds = vla.language_model.get_input_embeddings()\n",
    "# save language model \n",
    "if not os.path.exists(\"logs/llama-bridge\"):\n",
    "    vla.language_model.save_pretrained(\"logs/llama-bridge\")\n",
    "    processor.save_pretrained(\"logs/llama-bridge\")\n",
    "\n",
    "# load language model with VLLM\n",
    "if hasattr(vla, \"language_model\"):\n",
    "    del vla.language_model\n",
    "vla.language_model = LLM(\"logs/llama-bridge\", trust_remote_code=True, gpu_memory_utilization=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A chat between a curious user and an artificial intelligence assistant.\n",
      "The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
      "USER: What action should the robot take to place the watermelon on the towel? ASSISTANT: TASK:\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"A chat between a curious user and an artificial intelligence assistant. \"\n",
    "    \"The assistant gives helpful, detailed, and polite answers to the user's questions.\"\n",
    ")\n",
    "t = CotTag.TASK.value\n",
    "def get_openvla_prompt(instruction: str, task) -> str:\n",
    "    return f\"{SYSTEM_PROMPT} USER: What action should the robot take to {instruction.lower()}? ASSISTANT: {task}\"\n",
    "INSTRUCTION = \"place the watermelon on the towel\"\n",
    "prompt = get_openvla_prompt(INSTRUCTION, t)\n",
    "image = Image.open(\"./test.png\")\n",
    "print(prompt.replace(\". \", \".\\n\"))\n",
    "# print(\"Image size:\", image.size)\n",
    "dataset_statistics_path = os.path.join(path_to_converted_ckpt, \"dataset_statistics.json\")\n",
    "if os.path.isfile(dataset_statistics_path):\n",
    "    with open(dataset_statistics_path, \"r\") as f:\n",
    "        norm_stats = json.load(f)\n",
    "    vla.norm_stats = norm_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "different prompt tests (not directly supported)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepare the inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sync prompts (one prompt per image )\n",
    "prompts = []\n",
    "for t in CotTag:\n",
    "    prompt_b = f\"{SYSTEM_PROMPT} USER: What action should the robot take to {INSTRUCTION.lower()}? ASSISTANT: {t.value}\"\n",
    "    prompts.append(prompt_b)\n",
    "\n",
    "# async prompts\n",
    "async_prompts = \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: What action should the robot take to place the watermelon on the towel? ASSISTANT: TASK: The task is to place the watermelon on the towel. The first step is to move the robotic arm towards the towel. PLAN: 1. Move to the right and forward. 2. Move down and grip the towel. 3. Move backward and up. 4. Move to the left. VISIBLE OBJECTS: the robot task [100, 1, 153, 105], the towel [160, 99, 220, 164], the towel [160, 99, 221, 165], table [20, 39, 239, 249], the robot task [100, 1, 154, 106] SUBTASK REASONING: The towel is to the right and slightly forward from the current robotic arm position. The robotic arm needs to move forward and up to reach the towel and grip it. SUBTASK: Move forward and up. MOVE REASONING: The robotic arm needs to move forward and up to reach the towel and grip it. MOVE: Move forward up. GRIPPER POSITION: [121, 91, 130, 87, 142, 87, 153, 88, 169, 95] ACTION: 塔瀬ܝĦ越ਿŸ\"\n",
    "# break async_prompts with CotTag keep value before the tag\n",
    "\n",
    "# prompts = []\n",
    "# for t in CotTag:\n",
    "#     if t == CotTag.PLAN:\n",
    "#         break\n",
    "#     prompts.append(async_prompts.split(t.value)[0] + t.value)\n",
    "#     # print(prompts[-1])\n",
    "    \n",
    "from transformers.utils import TensorType\n",
    "# left padding\n",
    "# processor.tokenizer.padding_side = 'left'\n",
    "inputs = [processor.tokenizer(p, return_tensors=TensorType.PYTORCH)['input_ids'].to(device) for p in prompts]\n",
    "pixel_values = processor.image_processor(image, return_tensors=TensorType.PYTORCH)[\"pixel_values\"].to(device, dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: What action should the robot take to place the watermelon on the towel? ASSISTANT: TASK:\",\n",
       " \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: What action should the robot take to place the watermelon on the towel? ASSISTANT: PLAN:\",\n",
       " \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: What action should the robot take to place the watermelon on the towel? ASSISTANT: VISIBLE OBJECTS:\",\n",
       " \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: What action should the robot take to place the watermelon on the towel? ASSISTANT: SUBTASK REASONING:\",\n",
       " \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: What action should the robot take to place the watermelon on the towel? ASSISTANT: SUBTASK:\",\n",
       " \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: What action should the robot take to place the watermelon on the towel? ASSISTANT: MOVE REASONING:\",\n",
       " \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: What action should the robot take to place the watermelon on the towel? ASSISTANT: MOVE:\",\n",
       " \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: What action should the robot take to place the watermelon on the towel? ASSISTANT: GRIPPER POSITION:\",\n",
       " \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: What action should the robot take to place the watermelon on the towel? ASSISTANT: ACTION:\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-27 16:56:07 preprocess.py:236] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 9/9 [00:00<00:00, 16.27it/s, est. speed input: 5185.15 toks/s, output: 247.68 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time: 656.1441040039062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(temperature=0, max_tokens=1000, stop_token_ids=[2])\n",
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)\n",
    "start.record()\n",
    "outputs = vla.vllm_inference(input_ids=inputs, pixel_values=pixel_values)\n",
    "end.record()\n",
    "torch.cuda.synchronize()\n",
    "print(\"Inference time:\", start.elapsed_time(end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Place the watermelon on the towel. PLAN: Move the\n",
      "(15484, 278, 16699, 837, 295, 265, 373, 278, 304, 20466, 29889, 16507, 2190, 29901, 25249, 278)\n",
      "Moving forward out of a stable position. VISIBLE OBJECT\n",
      "(14104, 292, 6375, 714, 310, 263, 13714, 2602, 29889, 478, 3235, 8979, 1307, 438, 29933, 17637)\n",
      "the robot task a red [82, 1, 144\n",
      "(278, 19964, 3414, 263, 2654, 518, 29947, 29906, 29892, 29871, 29896, 29892, 29871, 29896, 29946, 29946)\n",
      "The gripper needs to move right and up to potentially reach the other side\n",
      "(450, 330, 374, 2496, 4225, 304, 4337, 1492, 322, 701, 304, 19998, 6159, 278, 916, 2625)\n",
      "Place the watermelon on the towel. SUBTASK:\n",
      "(15484, 278, 16699, 837, 295, 265, 373, 278, 304, 20466, 29889, 27092, 29911, 3289, 29968, 29901)\n",
      "Theovel is final, so the robot needs to move right to place it\n",
      "(450, 586, 295, 338, 2186, 29892, 577, 278, 19964, 4225, 304, 4337, 1492, 304, 2058, 372)\n",
      "MOVE_TO_WATMENASONING: Push the\n",
      "(16999, 12064, 29918, 4986, 29918, 29956, 1299, 29924, 1430, 29909, 3094, 4214, 29901, 349, 1878, 278)\n",
      "[130, 94, 143, 9\n",
      "(518, 29896, 29941, 29900, 29892, 29871, 29929, 29946, 29892, 29871, 29896, 29946, 29941, 29892, 29871, 29929)\n",
      "들故丸만々명Ÿ\n",
      "(29871, 31804, 31969, 31818, 31826, 31765, 31976, 31872, 2)\n"
     ]
    }
   ],
   "source": [
    "for o , p in zip(outputs, prompts):\n",
    "    # print(p + ' ' + o.outputs[0].text)\n",
    "    print(o.outputs[0].text)\n",
    "    print(o.outputs[0].token_ids)\n",
    "    # generated_text = o.outputs[0].token_ids\n",
    "    # print(generated_text)\n",
    "    # print(len(generated_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  18729 MiB |  18984 MiB | 107511 MiB |  88781 MiB |\n",
      "|       from large pool |  18569 MiB |  18819 MiB | 103212 MiB |  84643 MiB |\n",
      "|       from small pool |    160 MiB |    170 MiB |   4298 MiB |   4138 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  18729 MiB |  18984 MiB | 107511 MiB |  88781 MiB |\n",
      "|       from large pool |  18569 MiB |  18819 MiB | 103212 MiB |  84643 MiB |\n",
      "|       from small pool |    160 MiB |    170 MiB |   4298 MiB |   4138 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |  18685 MiB |  18940 MiB | 106016 MiB |  87330 MiB |\n",
      "|       from large pool |  18525 MiB |  18775 MiB | 101717 MiB |  83192 MiB |\n",
      "|       from small pool |    160 MiB |    170 MiB |   4298 MiB |   4138 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  19142 MiB |  19142 MiB |  32586 MiB |  13444 MiB |\n",
      "|       from large pool |  18928 MiB |  18928 MiB |  32348 MiB |  13420 MiB |\n",
      "|       from small pool |    214 MiB |    214 MiB |    238 MiB |     24 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  86135 KiB | 371200 KiB |  72761 MiB |  72677 MiB |\n",
      "|       from large pool |  41680 KiB | 368936 KiB |  66887 MiB |  66846 MiB |\n",
      "|       from small pool |  44455 KiB |  45128 KiB |   5874 MiB |   5830 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |    1643    |    1673    |   31317    |   29674    |\n",
      "|       from large pool |     540    |     547    |   14047    |   13507    |\n",
      "|       from small pool |    1103    |    1132    |   17270    |   16167    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |    1643    |    1673    |   31317    |   29674    |\n",
      "|       from large pool |     540    |     547    |   14047    |   13507    |\n",
      "|       from small pool |    1103    |    1132    |   17270    |   16167    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     370    |     370    |     629    |     259    |\n",
      "|       from large pool |     263    |     263    |     510    |     247    |\n",
      "|       from small pool |     107    |     107    |     119    |      12    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |     129    |     129    |   13471    |   13342    |\n",
      "|       from large pool |      24    |      33    |    8246    |    8222    |\n",
      "|       from small pool |     105    |     105    |    5225    |    5120    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#profile memroy with torch\n",
    "print(torch.cuda.memory_summary(device))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm-v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

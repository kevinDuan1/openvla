{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhekai/miniforge3/envs/vllm-v1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoConfig, AutoImageProcessor, AutoModelForVision2Seq, AutoProcessor\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import textwrap\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import enum\n",
    "import json\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define some utils.\n",
    "def split_reasoning(text, tags):\n",
    "    new_parts = {None: text}\n",
    "\n",
    "    for tag in tags:\n",
    "        parts = new_parts\n",
    "        new_parts = dict()\n",
    "\n",
    "        for k, v in parts.items():\n",
    "            if tag in v:\n",
    "                s = v.split(tag)\n",
    "                new_parts[k] = s[0]\n",
    "                new_parts[tag] = s[1]\n",
    "                # print(tag, s)\n",
    "            else:\n",
    "                new_parts[k] = v\n",
    "\n",
    "    return new_parts\n",
    "\n",
    "class CotTag(enum.Enum):\n",
    "    TASK = \"TASK:\"\n",
    "    PLAN = \"PLAN:\"\n",
    "    VISIBLE_OBJECTS = \"VISIBLE OBJECTS:\"\n",
    "    SUBTASK_REASONING = \"SUBTASK REASONING:\"\n",
    "    SUBTASK = \"SUBTASK:\"\n",
    "    MOVE_REASONING = \"MOVE REASONING:\"\n",
    "    MOVE = \"MOVE:\"\n",
    "    GRIPPER_POSITION = \"GRIPPER POSITION:\"\n",
    "    ACTION = \"ACTION:\"\n",
    "\n",
    "\n",
    "def get_cot_tags_list():\n",
    "    return [\n",
    "        CotTag.TASK.value,\n",
    "        CotTag.PLAN.value,\n",
    "        CotTag.VISIBLE_OBJECTS.value,\n",
    "        CotTag.SUBTASK_REASONING.value,\n",
    "        CotTag.SUBTASK.value,\n",
    "        CotTag.MOVE_REASONING.value,\n",
    "        CotTag.MOVE.value,\n",
    "        CotTag.GRIPPER_POSITION.value,\n",
    "        CotTag.ACTION.value,\n",
    "    ]\n",
    "\n",
    "def name_to_random_color(name):\n",
    "    return [(hash(name) // (256**i)) % 256 for i in range(3)]\n",
    "\n",
    "\n",
    "def draw_gripper(img, pos_list, img_size=(640, 480)):\n",
    "    for i, pos in enumerate(reversed(pos_list)):\n",
    "        pos = resize_pos(pos, img_size)\n",
    "        scale = 255 - int(255 * i / len(pos_list))\n",
    "        cv2.circle(img, pos, 6, (0, 0, 0), -1)\n",
    "        cv2.circle(img, pos, 5, (scale, scale, 255), -1)\n",
    "\n",
    "def get_metadata(reasoning):\n",
    "    metadata = {\"gripper\": [[0, 0]], \"bboxes\": dict()}\n",
    "\n",
    "    if f\" {CotTag.GRIPPER_POSITION.value}\" in reasoning:\n",
    "        gripper_pos = reasoning[f\" {CotTag.GRIPPER_POSITION.value}\"]\n",
    "        gripper_pos = gripper_pos.split(\"[\")[-1]\n",
    "        gripper_pos = gripper_pos.split(\"]\")[0]\n",
    "        gripper_pos = [int(x) for x in gripper_pos.split(\",\")]\n",
    "        gripper_pos = [(gripper_pos[2 * i], gripper_pos[2 * i + 1]) for i in range(len(gripper_pos) // 2)]\n",
    "        metadata[\"gripper\"] = gripper_pos\n",
    "\n",
    "    if f\" {CotTag.VISIBLE_OBJECTS.value}\" in reasoning:\n",
    "        for sample in reasoning[f\" {CotTag.VISIBLE_OBJECTS.value}\"].split(\"]\"):\n",
    "            obj = sample.split(\"[\")[0]\n",
    "            if obj == \"\":\n",
    "                continue\n",
    "            coords = [int(n) for n in sample.split(\"[\")[-1].split(\",\")]\n",
    "            metadata[\"bboxes\"][obj] = coords\n",
    "\n",
    "    return metadata\n",
    "\n",
    "def resize_pos(pos, img_size):\n",
    "    return [(x * size) // 256 for x, size in zip(pos, img_size)]\n",
    "\n",
    "def draw_bboxes(img, bboxes, img_size=(640, 480)):\n",
    "    for name, bbox in bboxes.items():\n",
    "        show_name = name\n",
    "        # show_name = f'{name}; {str(bbox)}'\n",
    "\n",
    "        cv2.rectangle(\n",
    "            img,\n",
    "            resize_pos((bbox[0], bbox[1]), img_size),\n",
    "            resize_pos((bbox[2], bbox[3]), img_size),\n",
    "            name_to_random_color(name),\n",
    "            1,\n",
    "        )\n",
    "        cv2.putText(\n",
    "            img,\n",
    "            show_name,\n",
    "            resize_pos((bbox[0], bbox[1] + 6), img_size),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.5,\n",
    "            (255, 255, 255),\n",
    "            1,\n",
    "            cv2.LINE_AA,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Expected `transformers==4.40.1` and `tokenizers==0.19.1` but got `transformers==4.48.1` and `tokenizers==0.21.0`; there might be inference-time regressions due to dependency changes. If in doubt, pleaseuse the above versions.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  8.13it/s]\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\"\n",
    "# Load Processor & VLA\n",
    "# path_to_converted_ckpt = \"Embodied-CoT/ecot-openvla-7b-bridge\"\n",
    "path_to_converted_ckpt = \"Embodied-CoT/ecot-openvla-7b-oxe\"\n",
    "# path_to_converted_ckpt = \"../openvla/logs/ecot-openvla-7b-oxe+libero_spatial_no_noops+b16+lr-0.0005+lora-r32+dropout-0.0--image_aug\"\n",
    "processor = AutoProcessor.from_pretrained(path_to_converted_ckpt, trust_remote_code=True)\n",
    "vla = AutoModelForVision2Seq.from_pretrained(\n",
    "    path_to_converted_ckpt,\n",
    "    # attn_implementation=\"flash_attention_2\",  # [Optional] Requires `flash_attn`\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-25 17:11:16 __init__.py:183] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 17:11:16,605\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-25 17:11:20 config.py:528] This model supports multiple tasks: {'classify', 'score', 'embed', 'reward', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 01-25 17:11:20 llm_engine.py:232] Initializing an LLM engine (v0.1.dev4262+g6609cdf) with config: model='logs/llama-bridge', speculative_config=None, tokenizer='logs/llama-bridge', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=logs/llama-bridge, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 01-25 17:11:21 cuda.py:225] Using Flash Attention backend.\n",
      "INFO 01-25 17:11:21 model_runner.py:1110] Starting to load model logs/llama-bridge...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:00<00:00,  2.44it/s]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:01<00:00,  1.93it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:01<00:00,  1.79it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:01<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-25 17:11:23 model_runner.py:1115] Loading model weights took 12.5527 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-25 17:11:24 worker.py:266] Memory profiling takes 0.77 seconds\n",
      "INFO 01-25 17:11:24 worker.py:266] the current vLLM instance can use total_gpu_memory (23.64GiB) x gpu_memory_utilization (0.80) = 18.91GiB\n",
      "INFO 01-25 17:11:24 worker.py:266] model weights take 12.55GiB; non_torch_memory takes 0.08GiB; PyTorch activation peak memory takes 0.44GiB; the rest of the memory reserved for KV Cache is 5.84GiB.\n",
      "INFO 01-25 17:11:24 executor_base.py:107] # CUDA blocks: 748, # CPU blocks: 512\n",
      "INFO 01-25 17:11:24 executor_base.py:112] Maximum concurrency for 2048 tokens per request: 5.84x\n",
      "INFO 01-25 17:11:25 model_runner.py:1448] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:11<00:00,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-25 17:11:37 model_runner.py:1574] Graph capturing finished in 11 secs, took 0.24 GiB\n",
      "INFO 01-25 17:11:37 llm_engine.py:429] init engine (profile, create kv cache, warmup model) took 13.45 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# vla.language_model.save_pretrained(\"logs/llama-bridge\")\n",
    "from vllm import LLM, SamplingParams\n",
    "sampling_params = SamplingParams(temperature=0, max_tokens=64,)\n",
    "vla.input_embds = vla.language_model.get_input_embeddings()\n",
    "\n",
    "# save language model \n",
    "if not os.path.exists(\"logs/llama-bridge\"):\n",
    "    vla.language_model.save_pretrained(\"logs/llama-bridge\")\n",
    "    processor.save_pretrained(\"logs/llama-bridge\")\n",
    "\n",
    "# load language model with VLLM\n",
    "if hasattr(vla, \"language_model\"):\n",
    "    del vla.language_model\n",
    "vla.language_model = LLM(\"logs/llama-bridge\", trust_remote_code=True, gpu_memory_utilization=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A chat between a curious user and an artificial intelligence assistant.\n",
      "The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
      "USER: What action should the robot take to place the watermelon on the towel? ASSISTANT: TASK:\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"A chat between a curious user and an artificial intelligence assistant. \"\n",
    "    \"The assistant gives helpful, detailed, and polite answers to the user's questions.\"\n",
    ")\n",
    "t = CotTag.TASK.value\n",
    "def get_openvla_prompt(instruction: str, task) -> str:\n",
    "    return f\"{SYSTEM_PROMPT} USER: What action should the robot take to {instruction.lower()}? ASSISTANT: {task}\"\n",
    "INSTRUCTION = \"place the watermelon on the towel\"\n",
    "prompt = get_openvla_prompt(INSTRUCTION, t)\n",
    "image = Image.open(\"./test.png\")\n",
    "print(prompt.replace(\". \", \".\\n\"))\n",
    "# print(\"Image size:\", image.size)\n",
    "dataset_statistics_path = os.path.join(path_to_converted_ckpt, \"dataset_statistics.json\")\n",
    "if os.path.isfile(dataset_statistics_path):\n",
    "    with open(dataset_statistics_path, \"r\") as f:\n",
    "        norm_stats = json.load(f)\n",
    "    vla.norm_stats = norm_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "different prompt tests (not directly supported)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepare the inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sync prompts (one prompt per image )\n",
    "prompts = []\n",
    "for t in CotTag:\n",
    "    prompt_b = f\"{SYSTEM_PROMPT} USER: What action should the robot take to {INSTRUCTION.lower()}? ASSISTANT: {t.value}\"\n",
    "    prompts.append(prompt_b)\n",
    "\n",
    "# async prompts\n",
    "async_prompts = \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: What action should the robot take to place the watermelon on the towel? ASSISTANT: TASK: The task is to place the watermelon on the towel. The first step is to move the robotic arm towards the towel. PLAN: 1. Move to the right and forward. 2. Move down and grip the towel. 3. Move backward and up. 4. Move to the left. VISIBLE OBJECTS: the robot task [100, 1, 153, 105], the towel [160, 99, 220, 164], the towel [160, 99, 221, 165], table [20, 39, 239, 249], the robot task [100, 1, 154, 106] SUBTASK REASONING: The towel is to the right and slightly forward from the current robotic arm position. The robotic arm needs to move forward and up to reach the towel and grip it. SUBTASK: Move forward and up. MOVE REASONING: The robotic arm needs to move forward and up to reach the towel and grip it. MOVE: Move forward up. GRIPPER POSITION: [121, 91, 130, 87, 142, 87, 153, 88, 169, 95] ACTION: 塔瀬ܝĦ越ਿŸ\"\n",
    "# break async_prompts with CotTag keep value before the tag\n",
    "\n",
    "prompts = []\n",
    "for t in CotTag:\n",
    "    # if t == CotTag.PLAN:\n",
    "    #     break\n",
    "    prompts.append(async_prompts.split(t.value)[0] + t.value)\n",
    "    # print(prompts[-1])\n",
    "    \n",
    "from transformers.utils import TensorType\n",
    "# left padding\n",
    "# processor.tokenizer.padding_side = 'left'\n",
    "inputs = [processor.tokenizer(p, return_tensors=TensorType.PYTORCH)['input_ids'].to(device) for p in prompts]\n",
    "pixel_values = processor.image_processor(image, return_tensors=TensorType.PYTORCH)[\"pixel_values\"].to(device, dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 9/9 [00:00<00:00, 12.59it/s, est. speed input: 6354.34 toks/s, output: 191.71 toks/s]\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(temperature=0,\n",
    "                                     max_tokens=64,\n",
    "                                     stop_token_ids=[2])\n",
    "outputs = vla.vllm_inference(input_ids=inputs, pixel_values=pixel_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: What action should the robot take to place the watermelon on the towel? ASSISTANT: TASK: The task is to place the watermelon on the towel. The\n",
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: What action should the robot take to place the watermelon on the towel? ASSISTANT: TASK: The task is to place the watermelon on the towel. The first step is to move the robotic arm towards the towel. PLAN: 1. Move to the right, above the towel. 2.\n",
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: What action should the robot take to place the watermelon on the towel? ASSISTANT: TASK: The task is to place the watermelon on the towel. The first step is to move the robotic arm towards the towel. PLAN: 1. Move to the right and forward. 2. Move down and grip the towel. 3. Move backward and up. 4. Move to the left. VISIBLE OBJECTS: a red spoon [70, 115, 10\n",
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: What action should the robot take to place the watermelon on the towel? ASSISTANT: TASK: The task is to place the watermelon on the towel. The first step is to move the robotic arm towards the towel. PLAN: 1. Move to the right and forward. 2. Move down and grip the towel. 3. Move backward and up. 4. Move to the left. VISIBLE OBJECTS: the robot task [100, 1, 153, 105], the towel [160, 99, 220, 164], the towel [160, 99, 221, 165], table [20, 39, 239, 249], the robot task [100, 1, 154, 106] SUBTASK REASONING: The robotic arm has gripped the towel, so it needs to\n",
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: What action should the robot take to place the watermelon on the towel? ASSISTANT: TASK: The task is to place the watermelon on the towel. The first step is to move the robotic arm towards the towel. PLAN: 1. Move to the right and forward. 2. Move down and grip the towel. 3. Move backward and up. 4. Move to the left. VISIBLE OBJECTS: the robot task [100, 1, 153, 105], the towel [160, 99, 220, 164], the towel [160, 99, 221, 165], table [20, 39, 239, 249], the robot task [100, 1, 154, 106] SUBTASK REASONING: The towel is to the right and slightly forward from the current robotic arm position. The robotic arm needs to move forward and up to reach the towel and grip it. SUBTASK: Move forward and up. MOVE REASONING: The grippers\n",
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: What action should the robot take to place the watermelon on the towel? ASSISTANT: TASK: The task is to place the watermelon on the towel. The first step is to move the robotic arm towards the towel. PLAN: 1. Move to the right and forward. 2. Move down and grip the towel. 3. Move backward and up. 4. Move to the left. VISIBLE OBJECTS: the robot task [100, 1, 153, 105], the towel [160, 99, 220, 164], the towel [160, 99, 221, 165], table [20, 39, 239, 249], the robot task [100, 1, 154, 106] SUBTASK REASONING: The towel is to the right and slightly forward from the current robotic arm position. The robotic arm needs to move forward and up to reach the towel and grip it. SUBTASK: Move forward and up. MOVE REASONING: The towel is to the right and slightly forward from the current robotic arm\n",
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: What action should the robot take to place the watermelon on the towel? ASSISTANT: TASK: The task is to place the watermelon on the towel. The first step is to move the robotic arm towards the towel. PLAN: 1. Move to the right and forward. 2. Move down and grip the towel. 3. Move backward and up. 4. Move to the left. VISIBLE OBJECTS: the robot task [100, 1, 153, 105], the towel [160, 99, 220, 164], the towel [160, 99, 221, 165], table [20, 39, 239, 249], the robot task [100, 1, 154, 106] SUBTASK REASONING: The towel is to the right and slightly forward from the current robotic arm position. The robotic arm needs to move forward and up to reach the towel and grip it. SUBTASK: Move forward and up. MOVE REASONING: The robotic arm needs to move forward and up to reach the towel and grip it. MOVE: Move forward up. GRIPPER POSITION: [115\n",
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: What action should the robot take to place the watermelon on the towel? ASSISTANT: TASK: The task is to place the watermelon on the towel. The first step is to move the robotic arm towards the towel. PLAN: 1. Move to the right and forward. 2. Move down and grip the towel. 3. Move backward and up. 4. Move to the left. VISIBLE OBJECTS: the robot task [100, 1, 153, 105], the towel [160, 99, 220, 164], the towel [160, 99, 221, 165], table [20, 39, 239, 249], the robot task [100, 1, 154, 106] SUBTASK REASONING: The towel is to the right and slightly forward from the current robotic arm position. The robotic arm needs to move forward and up to reach the towel and grip it. SUBTASK: Move forward and up. MOVE REASONING: The robotic arm needs to move forward and up to reach the towel and grip it. MOVE: Move forward up. GRIPPER POSITION: [116, 98, 126, 8\n",
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: What action should the robot take to place the watermelon on the towel? ASSISTANT: TASK: The task is to place the watermelon on the towel. The first step is to move the robotic arm towards the towel. PLAN: 1. Move to the right and forward. 2. Move down and grip the towel. 3. Move backward and up. 4. Move to the left. VISIBLE OBJECTS: the robot task [100, 1, 153, 105], the towel [160, 99, 220, 164], the towel [160, 99, 221, 165], table [20, 39, 239, 249], the robot task [100, 1, 154, 106] SUBTASK REASONING: The towel is to the right and slightly forward from the current robotic arm position. The robotic arm needs to move forward and up to reach the towel and grip it. SUBTASK: Move forward and up. MOVE REASONING: The robotic arm needs to move forward and up to reach the towel and grip it. MOVE: Move forward up. GRIPPER POSITION: [121, 91, 130, 87, 142, 87, 153, 88, 169, 95] ACTION: ܝ给拳塔微貴Ÿ\n"
     ]
    }
   ],
   "source": [
    "for o , p in zip(outputs, prompts):\n",
    "    print(p + ' ' + o.outputs[0].text)\n",
    "    # generated_text = o.outputs[0].token_ids\n",
    "    # print(generated_text)\n",
    "    # print(len(generated_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use cuda timmer \n",
    "# start = torch.cuda.Event(enable_timing=True)\n",
    "# end = torch.cuda.Event(enable_timing=True)\n",
    "# start.record()\n",
    "# action, generated_ids = vla.predict_action(**inputs, unnorm_key='libero_spatial_no_noops', do_sample=False, max_new_tokens=60)\n",
    "# end.record()\n",
    "# torch.cuda.synchronize()\n",
    "# print(start.elapsed_time(end))\n",
    "# generated_text = processor.batch_decode(generated_ids)\n",
    "# for i in range(len(prompts)):\n",
    "#     # print(f\"Prompt: {prompts[i]}\")\n",
    "#     print(f\"Generated: {generated_text[i]}\")\n",
    "#     print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#profile memroy with torch\n",
    "# print(torch.cuda.memory_summary(device))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm-v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

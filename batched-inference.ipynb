{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhekai/miniforge3/envs/openvla/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-01-15 13:19:42.466389: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-15 13:19:42.492627: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-01-15 13:19:42.492648: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-01-15 13:19:42.493491: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-15 13:19:42.498337: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-15 13:19:43.078602: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoConfig, AutoImageProcessor, AutoModelForVision2Seq, AutoProcessor\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import textwrap\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import enum\n",
    "import json\n",
    "import os \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define some utils.\n",
    "\n",
    "def split_reasoning(text, tags):\n",
    "    new_parts = {None: text}\n",
    "\n",
    "    for tag in tags:\n",
    "        parts = new_parts\n",
    "        new_parts = dict()\n",
    "\n",
    "        for k, v in parts.items():\n",
    "            if tag in v:\n",
    "                s = v.split(tag)\n",
    "                new_parts[k] = s[0]\n",
    "                new_parts[tag] = s[1]\n",
    "                # print(tag, s)\n",
    "            else:\n",
    "                new_parts[k] = v\n",
    "\n",
    "    return new_parts\n",
    "\n",
    "class CotTag(enum.Enum):\n",
    "    TASK = \"TASK:\"\n",
    "    PLAN = \"PLAN:\"\n",
    "    VISIBLE_OBJECTS = \"VISIBLE OBJECTS:\"\n",
    "    SUBTASK_REASONING = \"SUBTASK REASONING:\"\n",
    "    SUBTASK = \"SUBTASK:\"\n",
    "    MOVE_REASONING = \"MOVE REASONING:\"\n",
    "    MOVE = \"MOVE:\"\n",
    "    GRIPPER_POSITION = \"GRIPPER POSITION:\"\n",
    "    ACTION = \"ACTION:\"\n",
    "\n",
    "\n",
    "def get_cot_tags_list():\n",
    "    return [\n",
    "        CotTag.TASK.value,\n",
    "        CotTag.PLAN.value,\n",
    "        CotTag.VISIBLE_OBJECTS.value,\n",
    "        CotTag.SUBTASK_REASONING.value,\n",
    "        CotTag.SUBTASK.value,\n",
    "        CotTag.MOVE_REASONING.value,\n",
    "        CotTag.MOVE.value,\n",
    "        CotTag.GRIPPER_POSITION.value,\n",
    "        CotTag.ACTION.value,\n",
    "    ]\n",
    "\n",
    "def name_to_random_color(name):\n",
    "    return [(hash(name) // (256**i)) % 256 for i in range(3)]\n",
    "\n",
    "\n",
    "def draw_gripper(img, pos_list, img_size=(640, 480)):\n",
    "    for i, pos in enumerate(reversed(pos_list)):\n",
    "        pos = resize_pos(pos, img_size)\n",
    "        scale = 255 - int(255 * i / len(pos_list))\n",
    "        cv2.circle(img, pos, 6, (0, 0, 0), -1)\n",
    "        cv2.circle(img, pos, 5, (scale, scale, 255), -1)\n",
    "\n",
    "def get_metadata(reasoning):\n",
    "    metadata = {\"gripper\": [[0, 0]], \"bboxes\": dict()}\n",
    "\n",
    "    if f\" {CotTag.GRIPPER_POSITION.value}\" in reasoning:\n",
    "        gripper_pos = reasoning[f\" {CotTag.GRIPPER_POSITION.value}\"]\n",
    "        gripper_pos = gripper_pos.split(\"[\")[-1]\n",
    "        gripper_pos = gripper_pos.split(\"]\")[0]\n",
    "        gripper_pos = [int(x) for x in gripper_pos.split(\",\")]\n",
    "        gripper_pos = [(gripper_pos[2 * i], gripper_pos[2 * i + 1]) for i in range(len(gripper_pos) // 2)]\n",
    "        metadata[\"gripper\"] = gripper_pos\n",
    "\n",
    "    if f\" {CotTag.VISIBLE_OBJECTS.value}\" in reasoning:\n",
    "        for sample in reasoning[f\" {CotTag.VISIBLE_OBJECTS.value}\"].split(\"]\"):\n",
    "            obj = sample.split(\"[\")[0]\n",
    "            if obj == \"\":\n",
    "                continue\n",
    "            coords = [int(n) for n in sample.split(\"[\")[-1].split(\",\")]\n",
    "            metadata[\"bboxes\"][obj] = coords\n",
    "\n",
    "    return metadata\n",
    "\n",
    "def resize_pos(pos, img_size):\n",
    "    return [(x * size) // 256 for x, size in zip(pos, img_size)]\n",
    "\n",
    "def draw_bboxes(img, bboxes, img_size=(640, 480)):\n",
    "    for name, bbox in bboxes.items():\n",
    "        show_name = name\n",
    "        # show_name = f'{name}; {str(bbox)}'\n",
    "\n",
    "        cv2.rectangle(\n",
    "            img,\n",
    "            resize_pos((bbox[0], bbox[1]), img_size),\n",
    "            resize_pos((bbox[2], bbox[3]), img_size),\n",
    "            name_to_random_color(name),\n",
    "            1,\n",
    "        )\n",
    "        cv2.putText(\n",
    "            img,\n",
    "            show_name,\n",
    "            resize_pos((bbox[0], bbox[1] + 6), img_size),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.5,\n",
    "            (255, 255, 255),\n",
    "            1,\n",
    "            cv2.LINE_AA,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "Expected `transformers==4.40.1` and `tokenizers==0.19.1` but got `transformers==4.46.1` and `tokenizers==0.20.1`; there might be inference-time regressions due to dependency changes. If in doubt, pleaseuse the above versions.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  6.06it/s]\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:1\"\n",
    "# Load Processor & VLA\n",
    "# path_to_converted_ckpt = \"Embodied-CoT/ecot-openvla-7b-bridge\"\n",
    "# path_to_converted_ckpt = \"Embodied-CoT/ecot-openvla-7b-oxe\"\n",
    "path_to_converted_ckpt = \"../openvla/logs/ecot-openvla-7b-oxe+libero_spatial_no_noops+b16+lr-0.0005+lora-r32+dropout-0.0--image_aug\"\n",
    "processor = AutoProcessor.from_pretrained(path_to_converted_ckpt, trust_remote_code=True)\n",
    "vla = AutoModelForVision2Seq.from_pretrained(\n",
    "    path_to_converted_ckpt,\n",
    "    attn_implementation=\"flash_attention_2\",  # [Optional] Requires `flash_attn`\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A chat between a curious user and an artificial intelligence assistant.\n",
      "The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
      "USER: What action should the robot take to place the watermelon on the towel? ASSISTANT: TASK:\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"A chat between a curious user and an artificial intelligence assistant. \"\n",
    "    \"The assistant gives helpful, detailed, and polite answers to the user's questions.\"\n",
    ")\n",
    "t = CotTag.TASK.value\n",
    "def get_openvla_prompt(instruction: str, task) -> str:\n",
    "    return f\"{SYSTEM_PROMPT} USER: What action should the robot take to {instruction.lower()}? ASSISTANT: {task}\"\n",
    "\n",
    "INSTRUCTION = \"place the watermelon on the towel\"\n",
    "prompt = get_openvla_prompt(INSTRUCTION, t)\n",
    "image = Image.open(\"./test.png\")\n",
    "print(prompt.replace(\". \", \".\\n\"))\n",
    "# print(\"Image size:\", image.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(prompt, image).to(device, dtype=torch.bfloat16)\n",
    "dataset_statistics_path = os.path.join(path_to_converted_ckpt, \"dataset_statistics.json\")\n",
    "if os.path.isfile(dataset_statistics_path):\n",
    "    with open(dataset_statistics_path, \"r\") as f:\n",
    "        norm_stats = json.load(f)\n",
    "    vla.norm_stats = norm_stats\n",
    "\n",
    "\n",
    "# action, generated_ids = vla.predict_action(**inputs, unnorm_key='libero_spatial_no_noops', do_sample=False, max_new_tokens=1024)\n",
    "# generated_text = processor.batch_decode(generated_ids)[0]\n",
    "# print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "different prompt tests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = []\n",
    "for t in CotTag:\n",
    "    prompt_b = f\"{SYSTEM_PROMPT} USER: What action should the robot take to {INSTRUCTION.lower()}? ASSISTANT: {t.value}\"\n",
    "    prompts.append(prompt_b)\n",
    "\n",
    "# left padding\n",
    "processor.tokenizer.padding_side = 'left'\n",
    "inputs = processor(prompts, image, padding=True).to(device, dtype=torch.bfloat16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2041.260009765625\n",
      "Generated: <PAD><PAD><PAD><PAD><PAD><s> A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: What action should the robot take to place the watermelon on the towel? ASSISTANT: TASK: The task is to place the watermelon on the towel. The first high-level step is to approach the table. PLAN: Move forward, rotate clockwise, move forward. VISIBLE OBJECTS: the robot task [101, 2,\n",
      "\n",
      "Generated: <PAD><PAD><PAD><PAD><PAD><PAD><s> A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: What action should the robot take to place the watermelon on the towel? ASSISTANT: PLAN: 1. Lift the arm. 2. Move towards the stove. 3. Rotate the arm to face the stove. 4. Move to the side of the stove. 5. Go up. 6. Move away. VISIBLE OB\n",
      "\n",
      "Generated: <s> A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: What action should the robot take to place the watermelon on the towel? ASSISTANT: VISIBLE OBJECTS: the robot task [100, 2, 153, 109], the robot task [100, 2, 154, 110], the robot task [100, 2, 154, 1\n",
      "\n",
      "Generated: <s> A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: What action should the robot take to place the watermelon on the towel? ASSISTANT: SUBTASK REASONING: The arm's gripper needs to be open before it can move away from the table. SUBTASK: Move backward MOVE REASONING: The arm needs to move backward to get away from the table and avoid hitting it. MOVE: Move backward G\n",
      "\n",
      "Generated: <PAD><PAD><PAD><PAD><s> A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: What action should the robot take to place the watermelon on the towel? ASSISTANT: SUBTASK: Place the watermelon on the towel. SUBTASK: Place the watermelon on the towel. MOVE REASONING: The towel is directly above the robot, so the robot needs to move up to place the watermelon on it.\n",
      "\n",
      "Generated: <PAD><PAD><s> A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: What action should the robot take to place the watermelon on the towel? ASSISTANT: MOVE REASONING: The robotic arm needs to move towards the right side of the table, while maintaining the gripper closed to hold the watermelon. MOVE: move right GRIPPER POSITION: [119, 82, 138, 8\n",
      "\n",
      "Generated: <PAD><PAD><PAD><PAD><PAD><PAD><s> A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: What action should the robot take to place the watermelon on the towel? ASSISTANT: MOVE: Move right up. GRIPPER POSITION: [121, 93, 139, 90, 153, 93, 169, 98, 189, 109] SUBT\n",
      "\n",
      "Generated: <PAD><s> A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: What action should the robot take to place the watermelon on the towel? ASSISTANT: GRIPPER POSITION: [125, 82, 119, 83, 115, 83, 115, 83, 115, 83] ACTION: ක番銀ĦĚਿŸ</s><PAD><PAD><PAD>\n",
      "\n",
      "Generated: <PAD><PAD><PAD><PAD><PAD><PAD><s> A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: What action should the robot take to place the watermelon on the towel? ASSISTANT: ACTION: 차反达Ħ反ĦŸ</s><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use cuda timmer \n",
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)\n",
    "start.record()\n",
    "action, generated_ids = vla.predict_action(**inputs, unnorm_key='libero_spatial_no_noops', do_sample=False, max_new_tokens=60)\n",
    "end.record()\n",
    "torch.cuda.synchronize()\n",
    "print(start.elapsed_time(end))\n",
    "generated_text = processor.batch_decode(generated_ids)\n",
    "for i in range(len(prompts)):\n",
    "    # print(f\"Prompt: {prompts[i]}\")\n",
    "    print(f\"Generated: {generated_text[i]}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 1                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  14405 MiB |  17833 MiB | 150436 MiB | 136031 MiB |\n",
      "|       from large pool |  14401 MiB |  17828 MiB | 144037 MiB | 129635 MiB |\n",
      "|       from small pool |      3 MiB |      7 MiB |   6399 MiB |   6395 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  14405 MiB |  17833 MiB | 150436 MiB | 136031 MiB |\n",
      "|       from large pool |  14401 MiB |  17828 MiB | 144037 MiB | 129635 MiB |\n",
      "|       from small pool |      3 MiB |      7 MiB |   6399 MiB |   6395 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |  14392 MiB |  17820 MiB | 149865 MiB | 135472 MiB |\n",
      "|       from large pool |  14388 MiB |  17815 MiB | 143473 MiB | 129084 MiB |\n",
      "|       from small pool |      3 MiB |      7 MiB |   6391 MiB |   6388 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  20792 MiB |  20792 MiB |  20792 MiB |      0 B   |\n",
      "|       from large pool |  20782 MiB |  20782 MiB |  20782 MiB |      0 B   |\n",
      "|       from small pool |     10 MiB |     10 MiB |     10 MiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  55924 KiB |    972 MiB | 100937 MiB | 100882 MiB |\n",
      "|       from large pool |  53640 KiB |    970 MiB |  94489 MiB |  94437 MiB |\n",
      "|       from small pool |   2284 KiB |      3 MiB |   6447 MiB |   6445 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |    1020    |    1168    |   85750    |   84730    |\n",
      "|       from large pool |     441    |     571    |    5635    |    5194    |\n",
      "|       from small pool |     579    |     600    |   80115    |   79536    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |    1020    |    1168    |   85750    |   84730    |\n",
      "|       from large pool |     441    |     571    |    5635    |    5194    |\n",
      "|       from small pool |     579    |     600    |   80115    |   79536    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     491    |     491    |     491    |       0    |\n",
      "|       from large pool |     486    |     486    |     486    |       0    |\n",
      "|       from small pool |       5    |       5    |       5    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      29    |     125    |   38821    |   38792    |\n",
      "|       from large pool |      25    |     114    |    3039    |    3014    |\n",
      "|       from small pool |       4    |      13    |   35782    |   35778    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#profile memroy with torch\n",
    "print(torch.cuda.memory_summary(device))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

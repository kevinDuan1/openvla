{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhekai/miniforge3/envs/openvla/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-11-04 14:26:25.263384: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-04 14:26:25.290570: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-04 14:26:25.290591: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-04 14:26:25.291304: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-04 14:26:25.296710: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-04 14:26:25.873285: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForVision2Seq, AutoProcessor, BitsAndBytesConfig\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os \n",
    "\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expected `transformers==4.40.1` and `tokenizers==0.19.1` but got `transformers==4.46.1` and `tokenizers==0.20.1`; there might be inference-time regressions due to dependency changes. If in doubt, pleaseuse the above versions.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.11it/s]\n",
      "We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\n"
     ]
    }
   ],
   "source": [
    "# Load Processor & VLA\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "# quantization_config = BitsAndBytesConfig(load_in_8bit=True, llm_int8_threshold=10.0)\n",
    "# path = '/cluster/nvme9a/dzk/'\n",
    "processor = AutoProcessor.from_pretrained(\"openvla/openvla-7b\", trust_remote_code=True)\n",
    "vla = AutoModelForVision2Seq.from_pretrained(\n",
    "    \"openvla/openvla-7b\", \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # attn_implementation=\"flash_attention_2\",  # [Optional] Requires `flash_attn`\n",
    "    low_cpu_mem_usage=True, \n",
    "    trust_remote_code=True,\n",
    "    quantization_config=quantization_config,\n",
    ")#.to(device)\n",
    "\n",
    "# Grab image input & format prompt\n",
    "image = Image.open(\"test.png\")\n",
    "instruction = \"put eggplant into pot\"\n",
    "prompt = f\"In: What action should the robot take to {instruction}?\\nOut:\"\n",
    "\n",
    "# Predict Action (7-DoF; un-normalize for BridgeData V2)\n",
    "inputs = processor(prompt, image).to(device, dtype=torch.bfloat16)\n",
    "action = vla.predict_action(**inputs, unnorm_key=\"bridge_orig\", do_sample=False)\n",
    "# Execute...\n",
    "# robot.act(action, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "# === BFLOAT16 MODE ===\n",
    "inputs = processor(prompt, image).to(device, dtype=torch.bfloat16)\n",
    "# inputs[\"input_ids\"] = inputs[\"input_ids\"][:, 1:]\n",
    "\n",
    "# Run OpenVLA Inference\n",
    "torch.manual_seed(0)\n",
    "\n",
    "def trace_handler(prof):\n",
    "    # print(prof.key_averages().table(\n",
    "    #     sort_by=\"self_cuda_time_total\", row_limit=-1))\n",
    "    prof.export_chrome_trace(\"tmp/test_trace_\" + str(prof.step_num) + \".json\")\n",
    "\n",
    "# with torch.profiler.profile(\n",
    "#     activities=[\n",
    "#         torch.profiler.ProfilerActivity.CPU,\n",
    "#         torch.profiler.ProfilerActivity.CUDA,\n",
    "#     ],\n",
    "#     schedule=torch.profiler.schedule(\n",
    "#         wait=1,\n",
    "#         warmup=1,\n",
    "#         active=1),\n",
    "#     on_trace_ready=trace_handler,\n",
    "#     with_stack=True,\n",
    "#     profile_memory=True,\n",
    "#     with_flops = True\n",
    "#     ) as p:\n",
    "#         for iter in range(3):\n",
    "#             action = vla.predict_action(**inputs, unnorm_key=\"bridge_orig\", do_sample=False)\n",
    "#             p.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time:  184.96997528076173\n",
      "Std:  2.2852206230163574\n"
     ]
    }
   ],
   "source": [
    "# profile latecy with cuda event\n",
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)\n",
    "times = []\n",
    "for i in range(10):\n",
    "    start.record()\n",
    "    action = vla.predict_action(**inputs, unnorm_key=\"bridge_orig\", do_sample=False)\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    times.append(start.elapsed_time(end))\n",
    "print(\"Average inference time: \", sum(times)/len(times))\n",
    "print(\"Std: \", torch.tensor(times).std().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   4478 MiB |   4762 MiB | 225576 MiB | 221098 MiB |\n",
      "|       from large pool |   4271 MiB |   4555 MiB | 219447 MiB | 215176 MiB |\n",
      "|       from small pool |    206 MiB |    210 MiB |   6128 MiB |   5921 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   4478 MiB |   4762 MiB | 225576 MiB | 221098 MiB |\n",
      "|       from large pool |   4271 MiB |   4555 MiB | 219447 MiB | 215176 MiB |\n",
      "|       from small pool |    206 MiB |    210 MiB |   6128 MiB |   5921 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |   4418 MiB |   4700 MiB | 223891 MiB | 219473 MiB |\n",
      "|       from large pool |   4211 MiB |   4493 MiB | 217773 MiB | 213561 MiB |\n",
      "|       from small pool |    206 MiB |    210 MiB |   6118 MiB |   5911 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   4882 MiB |   4882 MiB |   8844 MiB |   3962 MiB |\n",
      "|       from large pool |   4670 MiB |   4670 MiB |   8632 MiB |   3962 MiB |\n",
      "|       from small pool |    212 MiB |    212 MiB |    212 MiB |      0 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory | 104344 KiB | 275200 KiB | 152626 MiB | 152524 MiB |\n",
      "|       from large pool | 103060 KiB | 274044 KiB | 145876 MiB | 145776 MiB |\n",
      "|       from small pool |   1284 KiB |   4333 KiB |   6749 MiB |   6748 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |    1891    |    2041    |  128514    |  126623    |\n",
      "|       from large pool |     486    |     614    |   23296    |   22810    |\n",
      "|       from small pool |    1405    |    1429    |  105218    |  103813    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |    1891    |    2041    |  128514    |  126623    |\n",
      "|       from large pool |     486    |     614    |   23296    |   22810    |\n",
      "|       from small pool |    1405    |    1429    |  105218    |  103813    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     289    |     289    |     362    |      73    |\n",
      "|       from large pool |     183    |     183    |     256    |      73    |\n",
      "|       from small pool |     106    |     106    |     106    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      67    |      82    |   59969    |   59902    |\n",
      "|       from large pool |      64    |      76    |   12807    |   12743    |\n",
      "|       from small pool |       3    |      11    |   47162    |   47159    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#profile memroy with torch\n",
    "print(torch.cuda.memory_summary(device))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
